## Context

The White project has three existing systems that need to be connected:

1. **Shrinkwrapped threads** (`shrinkwrapped/`) — 29 threads, each with song proposals per color containing key, bpm, time signature, concept text, genres, and moods
2. **Chord prototype** (`app/generators/midi/prototype/`) — Markov chain generator with 1,594 chords, function transition graphs (93 nodes, 588 edges), brute-force generation (1000 candidates in 0.5s), and music theory scoring
3. **ChromaticScorer** (`training/chromatic_scorer.py`) — ONNX-based scorer that evaluates MIDI against chromatic concepts (90%+ accuracy across temporal/spatial/ontological dimensions)

The pipeline connects these into a workflow where the human is the final decision-maker at each phase gate.

## Goals / Non-Goals

**Goals:**
- End-to-end workflow from song proposal → scored chord candidates → human-approved chords
- Composite scoring that combines music theory quality AND chromatic fitness
- YAML-based review interface that the human edits to approve/label candidates
- MIDI file output for each candidate so the human can listen
- Reproducible: re-running with same seed produces same candidates

**Non-Goals:**
- Audio rendering (user opens MIDI in Logic Pro or uses a separate tool)
- Real-time or interactive generation
- Multi-song batch processing
- Automated approval (human is always in the loop)

## Decisions

### Decision: Composite scoring (music theory + chromatic)

The chord prototype already has 4 scoring metrics (melody, voice leading, variety, graph probability). The ChromaticScorer adds 3 chromatic dimensions + confidence. These need to be combined into a single ranking.

**Approach:** Weighted composite score with configurable weights.

```
composite = w_theory * theory_score + w_chromatic * chromatic_score

theory_score = weighted_sum(melody, voice_leading, variety, graph_probability)
chromatic_score = weighted_sum(temporal_match, spatial_match, ontological_match, confidence)
```

Default weights emphasize chromatic fitness (the whole point) while using music theory as a quality floor:
- `w_theory = 0.3` — ensures candidates are musically coherent
- `w_chromatic = 0.7` — primary selection criterion

The user can adjust weights per song proposal if needed.

**Alternatives considered:**
- ChromaticScorer only → rejected because it doesn't evaluate music theory quality
- Two-stage filter (theory floor, then chromatic sort) → simpler but loses nuance; a candidate with exceptional chromatic match but slightly below-average theory score should still be considered
- Pareto ranking → more complex than needed for this iteration

### Decision: Chromatic target from song proposal color

Each song proposal has a `rainbow_color` (e.g., Black, Red, Green). The chromatic target is derived from the color's known mode distribution:

| Color   | Temporal | Spatial | Ontological |
|---------|----------|---------|-------------|
| Black   | —        | —       | —           |
| Red     | Past     | Thing   | Known       |
| Orange  | Past     | Place   | Imagined    |
| Yellow  | Present  | Place   | Known       |
| Green   | Present  | Place   | Forgotten   |
| Blue    | Past     | Person  | Known       |
| Indigo  | Future   | Thing   | Forgotten   |
| Violet  | Present  | Person  | Imagined    |
| White   | all      | all     | all         |

For scoring, we convert the target mode to a soft distribution (e.g., Red temporal = [0.8, 0.1, 0.1] for [Past, Present, Future]) and measure how well the scorer's output matches. White songs use uniform targets.

### Decision: YAML review files

Each generation run produces a review YAML file in the song's working directory. The human edits this file to label and approve candidates.

```yaml
# Auto-generated by chord pipeline
song_proposal: song_proposal_Black (0x221f20)_sequential_dissolution_v2.yml
thread: white-the-breathing-machine-learns-to-sing
generated: 2026-02-15T10:30:00
seed: 42
scoring_weights:
  theory: 0.3
  chromatic: 0.7

candidates:
  - id: chord_001
    midi_file: candidates/chord_001.mid
    rank: 1
    scores:
      composite: 0.847
      theory: {melody: 0.9, voice_leading: 0.85, variety: 0.75, graph_probability: 0.6}
      chromatic: {temporal: {past: 0.82}, spatial: {thing: 0.71}, ontological: {known: 0.88}, confidence: 0.91}
    progression: "i - bVI - III - bVII (F#m - D - A - E)"
    # ---- Human fills in below ----
    label: null        # verse-candidate | chorus-candidate | bridge-candidate | intro-candidate | outro-candidate
    status: pending    # pending | approved | rejected
    notes: ""

  - id: chord_002
    # ...
```

**Why YAML over a web UI:** The user already works with YAML manifests throughout the project. No new tooling to build. Can be version-controlled. Claude can read and process these files.

### Decision: Working directory per song

Each song gets a working directory under the shrinkwrapped thread:

```
shrinkwrapped/white-the-breathing-machine-learns-to-sing/
├── manifest.yml
├── yml/song_proposal_Black...yml
├── production/                          # NEW
│   └── black_sequential_dissolution/    # per song
│       ├── chords/
│       │   ├── review.yml               # human reviews here
│       │   ├── candidates/
│       │   │   ├── chord_001.mid
│       │   │   ├── chord_002.mid
│       │   │   └── ...
│       │   └── approved/                # approved candidates copied here
│       │       ├── verse.mid
│       │       └── chorus.mid
│       ├── drums/                       # future
│       └── assembly/                    # future
```

### Decision: Hybrid generation strategy

The existing Markov chain generator is the primary source. But we add a **Claude-suggested structure** option:

1. **Graph-guided Markov** (default, 80% of candidates) — uses the function transition graph to generate theory-grounded progressions in the target key
2. **Claude-suggested** (20% of candidates) — Claude reads the concept text and proposes harmonic structures (e.g., "for this dark, subversive Black concept: try i-bVI-III-bVII, or i-iv-bVI-V"), then the Markov chain generates voicing variants within those structures

This is not mandatory for v1 — pure Markov is sufficient. But it's architecturally easy to add since the generator already supports starting from a function sequence.

## Risks / Trade-offs

- **ChromaticScorer trained on existing music, not generated music** — The scorer learned from real recordings. Generated MIDI may have different characteristics. The scorer's confidence output should flag when it's uncertain.
- **Markov chain limited to existing chord vocabulary** — 1,594 chords from the MIDI pack. May miss unusual voicings. Acceptable for v1.
- **No audio rendering** — Human must open MIDI files in a DAW to listen. Could add FluidSynth rendering later.
- **YAML editing is manual** — If the user wants to review 50+ candidates, YAML gets unwieldy. Top-10 filtering before review file generation mitigates this.

## Open Questions

1. **How many candidates in the review file?** Proposed: top 10 (user reviews 10, not 50). The remaining 40+ are still generated for scoring diversity.
2. **Should we render MIDI to audio automatically?** FluidSynth can produce basic audio, but quality is low. Alternatively, the user drops MIDI files into Logic Pro. Propose: defer to future iteration.
3. **Claude-suggested structures** — include in v1 or defer? Propose: defer, note as enhancement.
