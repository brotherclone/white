{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 8: Model Interpretability Analysis\n",
    "\n",
    "Understand what the multiclass rebracketing classifier learned.\n",
    "\n",
    "**Sections:**\n",
    "1. Load trained model\n",
    "2. Embedding space visualization (TSNE/UMAP)\n",
    "3. Attention visualization\n",
    "4. Feature attribution (which words matter)\n",
    "5. Chromatic geometry analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install interpretability dependencies\n",
    "# !pip install captum umap-learn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device (CPU is fine for analysis)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "CHECKPOINT_PATH = \"output/checkpoint_best.pt\"\n",
    "DATA_PATH = \"data/base_manifest_db.parquet\"\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "\n",
    "print(f\"Checkpoint keys: {checkpoint.keys()}\")\n",
    "print(f\"Target type: {checkpoint.get('target_type', 'unknown')}\")\n",
    "print(f\"Num classes: {checkpoint.get('num_classes', 'unknown')}\")\n",
    "print(f\"Class mapping: {checkpoint.get('class_mapping', {})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild model from checkpoint config\n",
    "from models.text_encoder import TextEncoder\n",
    "from models.multiclass_classifier import MultiClassRebracketingClassifier, MultiClassRainbowModel\n",
    "\n",
    "config = checkpoint[\"config\"]\n",
    "class_mapping = checkpoint[\"class_mapping\"]\n",
    "num_classes = checkpoint[\"num_classes\"]\n",
    "class_names = list(class_mapping.keys())\n",
    "\n",
    "# Text encoder\n",
    "text_config = config[\"model\"][\"text_encoder\"]\n",
    "text_encoder = TextEncoder(\n",
    "    model_name=text_config[\"model_name\"],\n",
    "    hidden_size=text_config[\"hidden_size\"],\n",
    "    freeze_layers=text_config[\"freeze_layers\"],\n",
    "    pooling=text_config[\"pooling\"],\n",
    ")\n",
    "\n",
    "# Classifier\n",
    "clf_config = config[\"model\"][\"classifier\"]\n",
    "classifier = MultiClassRebracketingClassifier(\n",
    "    input_dim=text_encoder.hidden_size,\n",
    "    num_classes=num_classes,\n",
    "    hidden_dims=clf_config[\"hidden_dims\"],\n",
    "    dropout=clf_config[\"dropout\"],\n",
    "    activation=clf_config[\"activation\"],\n",
    ")\n",
    "\n",
    "# Combined model\n",
    "model = MultiClassRainbowModel(text_encoder=text_encoder, classifier=classifier)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"\\nModel loaded successfully!\")\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    text_config[\"model_name\"],\n",
    "    use_fast=False,\n",
    "    add_prefix_space=False,\n",
    ")\n",
    "print(f\"Tokenizer loaded: {text_config['model_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "\n",
    "# Filter to rows with concepts and known rebracketing types\n",
    "df = df[df[\"concept\"].notna()]\n",
    "df[\"rebracketing_type\"] = df[\"training_data\"].apply(\n",
    "    lambda x: x.get(\"rebracketing_type\") if isinstance(x, dict) else None\n",
    ")\n",
    "df = df[df[\"rebracketing_type\"].isin(class_mapping.keys())]\n",
    "\n",
    "print(f\"Loaded {len(df)} samples with known rebracketing types\")\n",
    "print(df[\"rebracketing_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding Space Visualization\n",
    "\n",
    "See how different rebracketing types cluster in the learned embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, batch_size=16):\n",
    "    \"\"\"Extract embeddings for a list of texts.\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=text_config[\"max_length\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get embeddings from text encoder (before classifier)\n",
    "            emb = model.text_encoder(input_ids, attention_mask)\n",
    "            embeddings.append(emb.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for all samples\n",
    "texts = df[\"concept\"].tolist()\n",
    "labels = df[\"rebracketing_type\"].tolist()\n",
    "colors = df.get(\"rainbow_color\", pd.Series([\"unknown\"] * len(df))).tolist()\n",
    "\n",
    "embeddings = get_embeddings(texts)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TSNE visualization\nfrom sklearn.manifold import TSNE\n\nprint(\"Running TSNE...\")\ntsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))\nembeddings_2d = tsne.fit_transform(embeddings)\n\n# Plot by rebracketing type\nfig, ax = plt.subplots(figsize=(12, 8))\n\nfor rb_type in class_names:\n    mask = [label == rb_type for label in labels]\n    if sum(mask) > 0:\n        ax.scatter(\n            embeddings_2d[mask, 0],\n            embeddings_2d[mask, 1],\n            label=f\"{rb_type} ({sum(mask)})\",\n            alpha=0.7,\n            s=50,\n        )\n\nax.set_xlabel(\"TSNE-1\")\nax.set_ylabel(\"TSNE-2\")\nax.set_title(\"Embedding Space by Rebracketing Type\")\nax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.savefig(\"output/embedding_tsne_rebracketing.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# UMAP visualization (often better than TSNE)\ntry:\n    import umap\n    \n    print(\"Running UMAP...\")\n    reducer = umap.UMAP(n_components=2, random_state=42)\n    embeddings_umap = reducer.fit_transform(embeddings)\n    \n    fig, ax = plt.subplots(figsize=(12, 8))\n    \n    for rb_type in class_names:\n        mask = [label == rb_type for label in labels]\n        if sum(mask) > 0:\n            ax.scatter(\n                embeddings_umap[mask, 0],\n                embeddings_umap[mask, 1],\n                label=f\"{rb_type} ({sum(mask)})\",\n                alpha=0.7,\n                s=50,\n            )\n    \n    ax.set_xlabel(\"UMAP-1\")\n    ax.set_ylabel(\"UMAP-2\")\n    ax.set_title(\"Embedding Space by Rebracketing Type (UMAP)\")\n    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.tight_layout()\n    plt.savefig(\"output/embedding_umap_rebracketing.png\", dpi=150, bbox_inches=\"tight\")\n    plt.show()\nexcept ImportError:\n    print(\"UMAP not installed. Run: pip install umap-learn\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot by chromatic color if available\n",
    "if \"rainbow_color\" in df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    color_map = {\n",
    "        \"BLACK\": \"black\",\n",
    "        \"RED\": \"red\",\n",
    "        \"ORANGE\": \"orange\",\n",
    "        \"YELLOW\": \"gold\",\n",
    "        \"GREEN\": \"green\",\n",
    "        \"BLUE\": \"blue\",\n",
    "        \"INDIGO\": \"indigo\",\n",
    "        \"VIOLET\": \"violet\",\n",
    "    }\n",
    "    \n",
    "    for color_name, plot_color in color_map.items():\n",
    "        mask = [c == color_name for c in colors]\n",
    "        if sum(mask) > 0:\n",
    "            ax.scatter(\n",
    "                embeddings_2d[mask, 0],\n",
    "                embeddings_2d[mask, 1],\n",
    "                label=f\"{color_name} ({sum(mask)})\",\n",
    "                color=plot_color,\n",
    "                alpha=0.7,\n",
    "                s=50,\n",
    "            )\n",
    "    \n",
    "    ax.set_xlabel(\"TSNE-1\")\n",
    "    ax.set_ylabel(\"TSNE-2\")\n",
    "    ax.set_title(\"Embedding Space by Chromatic Color\")\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/embedding_tsne_chromatic.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attention Visualization\n",
    "\n",
    "See which words the model focuses on for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_weights(text):\n",
    "    \"\"\"Extract attention weights for a single text.\"\"\"\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=text_config[\"max_length\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get attention from the underlying transformer\n",
    "        outputs = model.text_encoder.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=True,\n",
    "        )\n",
    "        \n",
    "        # Get prediction\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        pred_idx = torch.argmax(logits, dim=-1).item()\n",
    "        pred_class = class_names[pred_idx]\n",
    "        \n",
    "    # Average attention across heads and layers\n",
    "    # Shape: (num_layers, batch, num_heads, seq_len, seq_len)\n",
    "    attentions = outputs.attentions\n",
    "    \n",
    "    # Take last layer, average over heads\n",
    "    last_layer_attn = attentions[-1][0].mean(dim=0)  # (seq_len, seq_len)\n",
    "    \n",
    "    # Get attention to [CLS] token (or average)\n",
    "    cls_attention = last_layer_attn[0].cpu().numpy()  # Attention from CLS to all tokens\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu())\n",
    "    \n",
    "    # Mask padding\n",
    "    seq_len = attention_mask.sum().item()\n",
    "    tokens = tokens[:seq_len]\n",
    "    cls_attention = cls_attention[:seq_len]\n",
    "    \n",
    "    return tokens, cls_attention, pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(text, figsize=(16, 4)):\n",
    "    \"\"\"Visualize attention weights for a text.\"\"\"\n",
    "    tokens, attention, pred_class = get_attention_weights(text)\n",
    "    \n",
    "    # Normalize attention\n",
    "    attention = (attention - attention.min()) / (attention.max() - attention.min() + 1e-8)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Create heatmap\n",
    "    im = ax.imshow([attention], cmap=\"YlOrRd\", aspect=\"auto\")\n",
    "    \n",
    "    # Set tokens as x labels\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha=\"right\", fontsize=8)\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    ax.set_title(f\"Attention Weights (Predicted: {pred_class})\")\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, label=\"Attention\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention for a few examples\n",
    "sample_indices = df.sample(min(5, len(df)), random_state=42).index\n",
    "\n",
    "for idx in sample_indices:\n",
    "    row = df.loc[idx]\n",
    "    concept = row[\"concept\"][:500]  # Truncate for visualization\n",
    "    true_type = row[\"rebracketing_type\"]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"True type: {true_type}\")\n",
    "    print(f\"Concept: {concept[:200]}...\")\n",
    "    \n",
    "    fig = visualize_attention(concept)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Attribution\n",
    "\n",
    "Which words drive the model's predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "try:\n    from captum.attr import LayerIntegratedGradients\n    CAPTUM_AVAILABLE = True\nexcept ImportError:\n    print(\"Captum not installed. Run: pip install captum\")\n    CAPTUM_AVAILABLE = False"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CAPTUM_AVAILABLE:\n",
    "    def forward_for_attribution(input_ids, attention_mask):\n",
    "        \"\"\"Forward function for Captum attribution.\"\"\"\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        return logits\n",
    "    \n",
    "    def get_attribution(text, target_class=None):\n",
    "        \"\"\"Get integrated gradients attribution for a text.\"\"\"\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            max_length=text_config[\"max_length\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Get prediction if target not specified\n",
    "        if target_class is None:\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids, attention_mask)\n",
    "                target_class = torch.argmax(logits, dim=-1).item()\n",
    "        \n",
    "        # Baseline: padding tokens\n",
    "        baseline_ids = torch.zeros_like(input_ids)\n",
    "        baseline_ids.fill_(tokenizer.pad_token_id)\n",
    "        \n",
    "        # Get embeddings layer for attribution\n",
    "        embeddings_layer = model.text_encoder.model.embeddings\n",
    "        \n",
    "        lig = LayerIntegratedGradients(forward_for_attribution, embeddings_layer)\n",
    "        \n",
    "        attributions = lig.attribute(\n",
    "            inputs=input_ids,\n",
    "            baselines=baseline_ids,\n",
    "            additional_forward_args=(attention_mask,),\n",
    "            target=target_class,\n",
    "            n_steps=50,\n",
    "        )\n",
    "        \n",
    "        # Sum over embedding dimension\n",
    "        attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "        attributions = attributions.cpu().numpy()\n",
    "        \n",
    "        # Get tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu())\n",
    "        seq_len = attention_mask.sum().item()\n",
    "        \n",
    "        return tokens[:seq_len], attributions[:seq_len], class_names[target_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CAPTUM_AVAILABLE:\n",
    "    def visualize_attribution(text, figsize=(16, 4)):\n",
    "        \"\"\"Visualize word importance via integrated gradients.\"\"\"\n",
    "        tokens, attributions, pred_class = get_attribution(text)\n",
    "        \n",
    "        # Normalize\n",
    "        attr_max = np.abs(attributions).max()\n",
    "        if attr_max > 0:\n",
    "            attributions = attributions / attr_max\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        colors = ['red' if a < 0 else 'green' for a in attributions]\n",
    "        ax.bar(range(len(tokens)), attributions, color=colors, alpha=0.7)\n",
    "        \n",
    "        ax.set_xticks(range(len(tokens)))\n",
    "        ax.set_xticklabels(tokens, rotation=45, ha=\"right\", fontsize=8)\n",
    "        ax.set_ylabel(\"Attribution\")\n",
    "        ax.set_title(f\"Word Importance (Predicted: {pred_class})\")\n",
    "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    # Visualize attribution for a few examples\n",
    "    for idx in sample_indices[:3]:\n",
    "        row = df.loc[idx]\n",
    "        concept = row[\"concept\"][:300]  # Shorter for attribution\n",
    "        true_type = row[\"rebracketing_type\"]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"True type: {true_type}\")\n",
    "        print(f\"Concept: {concept[:150]}...\")\n",
    "        \n",
    "        fig = visualize_attribution(concept)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Class Predictions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for all samples\n",
    "def get_predictions(texts, batch_size=16):\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Getting predictions\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=text_config[\"max_length\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_probs)\n",
    "\n",
    "predictions, probabilities = get_predictions(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Confusion matrix\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Convert labels to indices\nlabel_indices = [class_mapping[lbl] for lbl in labels]\n\ncm = confusion_matrix(label_indices, predictions, labels=list(range(num_classes)))\n\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(\n    cm,\n    annot=True,\n    fmt=\"d\",\n    cmap=\"Blues\",\n    xticklabels=class_names,\n    yticklabels=class_names,\n    ax=ax,\n)\nax.set_xlabel(\"Predicted\")\nax.set_ylabel(\"True\")\nax.set_title(\"Confusion Matrix\")\nplt.tight_layout()\nplt.savefig(\"output/confusion_matrix_analysis.png\", dpi=150)\nplt.show()\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(label_indices, predictions, target_names=class_names, zero_division=0))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction confidence distribution\n",
    "max_probs = probabilities.max(axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Overall confidence distribution\n",
    "axes[0].hist(max_probs, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel(\"Prediction Confidence\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Confidence Distribution\")\n",
    "axes[0].axvline(x=0.5, color='red', linestyle='--', label='50%')\n",
    "axes[0].legend()\n",
    "\n",
    "# Confidence by correctness\n",
    "correct = predictions == np.array(label_indices)\n",
    "axes[1].hist(max_probs[correct], bins=20, alpha=0.7, label=f\"Correct ({correct.sum()})\")\n",
    "axes[1].hist(max_probs[~correct], bins=20, alpha=0.7, label=f\"Incorrect ({(~correct).sum()})\")\n",
    "axes[1].set_xlabel(\"Prediction Confidence\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"Confidence by Correctness\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/confidence_distribution.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Misclassification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "df[\"predicted\"] = [class_names[p] for p in predictions]\n",
    "df[\"confidence\"] = max_probs\n",
    "df[\"correct\"] = df[\"predicted\"] == df[\"rebracketing_type\"]\n",
    "\n",
    "misclassified = df[~df[\"correct\"]]\n",
    "\n",
    "print(f\"Misclassified: {len(misclassified)} / {len(df)} ({100*len(misclassified)/len(df):.1f}%)\")\n",
    "print(\"\\nMost common confusions:\")\n",
    "print(misclassified.groupby([\"rebracketing_type\", \"predicted\"]).size().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some misclassified examples\n",
    "print(\"\\nExample misclassifications:\")\n",
    "for _, row in misclassified.head(5).iterrows():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"True: {row['rebracketing_type']} | Predicted: {row['predicted']} | Conf: {row['confidence']:.2f}\")\n",
    "    print(f\"Concept: {row['concept'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key findings from this analysis:\n",
    "\n",
    "1. **Embedding Space**: How well do rebracketing types cluster?\n",
    "2. **Attention**: What words does the model focus on?\n",
    "3. **Attribution**: Which words drive predictions?\n",
    "4. **Confusions**: Where does the model struggle?\n",
    "\n",
    "Save this notebook's outputs for documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}