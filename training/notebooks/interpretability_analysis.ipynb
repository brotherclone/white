{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 8: Model Interpretability Analysis\n",
    "\n",
    "# Version 6\n",
    "\n",
    "Understand what the multiclass rebracketing classifier learned.\n",
    "\n",
    "**Sections:**\n",
    "1. Setup & Load from HuggingFace\n",
    "2. Embedding space visualization (TSNE/UMAP)\n",
    "3. Class predictions & confusion matrix\n",
    "4. Misclassification analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Install dependencies\n# After this cell completes, do: Runtime -> Restart session\n# Then SKIP this cell and continue from the next one\n\n!pip install -q transformers datasets huggingface_hub wandb umap-learn\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"RESTART RUNTIME NOW: Runtime -> Restart session\")\nprint(\"Then SKIP this cell and run from the next cell\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# HuggingFace\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CHECKPOINT_PATH = \"/content/drive/MyDrive/Colab Notebooks/checkpoint_best.pt\"\n",
    "HF_DATASET = \"earthlyframes/white-rebracketing\"  # Your HF dataset\n",
    "WANDB_PROJECT = \"White\"\n",
    "\n",
    "# Initialize wandb (optional - set to False to skip)\n",
    "USE_WANDB = True\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.login()\n",
    "    run = wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        job_type=\"interpretability\",\n",
    "        config={\n",
    "            \"checkpoint\": CHECKPOINT_PATH,\n",
    "            \"dataset\": HF_DATASET,\n",
    "        }\n",
    "    )\n",
    "    print(f\"W&B run: {run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "\n",
    "print(f\"Checkpoint keys: {checkpoint.keys()}\")\n",
    "print(f\"Target type: {checkpoint.get('target_type', 'unknown')}\")\n",
    "print(f\"Num classes: {checkpoint.get('num_classes', 'unknown')}\")\n",
    "print(f\"Class mapping: {checkpoint.get('class_mapping', {})}\")\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.config.update({\n",
    "        \"num_classes\": checkpoint.get('num_classes'),\n",
    "        \"class_mapping\": checkpoint.get('class_mapping'),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract config from checkpoint\n",
    "config = checkpoint[\"config\"]\n",
    "class_mapping = checkpoint[\"class_mapping\"]\n",
    "num_classes = checkpoint[\"num_classes\"]\n",
    "class_names = list(class_mapping.keys())\n",
    "\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture (must match training exactly)\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Text encoder using pretrained transformer.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, hidden_size=None, freeze_layers=0, pooling=\"cls\", **kwargs):\n",
    "        super().__init__()\n",
    "        self.pooling = pooling\n",
    "        \n",
    "        # Load the transformer - use 'encoder' not 'model' to match checkpoint\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.encoder.config.hidden_size\n",
    "        \n",
    "        # Freeze layers if specified\n",
    "        if freeze_layers and freeze_layers > 0:\n",
    "            if hasattr(self.encoder, \"embeddings\"):\n",
    "                for param in self.encoder.embeddings.parameters():\n",
    "                    param.requires_grad = False\n",
    "            if hasattr(self.encoder, \"encoder\") and hasattr(self.encoder.encoder, \"layer\"):\n",
    "                for i, layer in enumerate(self.encoder.encoder.layer):\n",
    "                    if i < freeze_layers:\n",
    "                        for param in layer.parameters():\n",
    "                            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        \n",
    "        if self.pooling == \"cls\":\n",
    "            pooled = sequence_output[:, 0, :]\n",
    "        elif self.pooling == \"mean\":\n",
    "            mask_expanded = attention_mask.unsqueeze(-1).expand_as(sequence_output).float()\n",
    "            sum_embeddings = torch.sum(sequence_output * mask_expanded, dim=1)\n",
    "            sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "            pooled = sum_embeddings / sum_mask\n",
    "        else:\n",
    "            pooled = sequence_output[:, 0, :]\n",
    "        \n",
    "        return pooled\n",
    "\n",
    "\n",
    "class MultiClassRebracketingClassifier(nn.Module):\n",
    "    \"\"\"Classifier head for rebracketing type prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, num_classes, hidden_dims=None, dropout=0.3, activation=\"relu\", **kwargs):\n",
    "        super().__init__()\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [256, 128]\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            if activation == \"gelu\":\n",
    "                layers.append(nn.GELU())\n",
    "            else:\n",
    "                layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        self.mlp = nn.Sequential(*layers)  # Use 'mlp' not 'classifier' to match checkpoint\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class MultiClassRainbowModel(nn.Module):\n",
    "    \"\"\"Combined text encoder + classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, text_encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.classifier = classifier\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embeddings = self.text_encoder(input_ids, attention_mask)\n",
    "        return self.classifier(embeddings)\n",
    "\n",
    "print(\"Model classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and load model\n",
    "text_config = config[\"model\"][\"text_encoder\"]\n",
    "clf_config = config[\"model\"][\"classifier\"]\n",
    "\n",
    "text_encoder = TextEncoder(\n",
    "    model_name=text_config[\"model_name\"],\n",
    "    hidden_size=text_config[\"hidden_size\"],\n",
    "    freeze_layers=text_config[\"freeze_layers\"],\n",
    "    pooling=text_config[\"pooling\"],\n",
    ")\n",
    "\n",
    "classifier = MultiClassRebracketingClassifier(\n",
    "    input_dim=text_encoder.hidden_size,\n",
    "    num_classes=num_classes,\n",
    "    hidden_dims=clf_config[\"hidden_dims\"],\n",
    "    dropout=clf_config[\"dropout\"],\n",
    "    activation=clf_config[\"activation\"],\n",
    ")\n",
    "\n",
    "model = MultiClassRainbowModel(text_encoder=text_encoder, classifier=classifier)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Text encoder: {text_config['model_name']}\")\n",
    "print(f\"Hidden size: {text_config['hidden_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    text_config[\"model_name\"],\n",
    "    use_fast=False,\n",
    "    add_prefix_space=False,\n",
    ")\n",
    "print(f\"Tokenizer loaded: {text_config['model_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from HuggingFace\n",
    "try:\n",
    "    dataset = load_dataset(HF_DATASET)\n",
    "    print(f\"Loaded dataset from HuggingFace: {HF_DATASET}\")\n",
    "    print(dataset)\n",
    "    \n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    if \"train\" in dataset:\n",
    "        df = dataset[\"train\"].to_pandas()\n",
    "    else:\n",
    "        df = dataset.to_pandas()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not load from HuggingFace: {e}\")\n",
    "    print(\"Falling back to local parquet...\")\n",
    "    df = pd.read_parquet(\"/content/drive/MyDrive/Colab Notebooks/data/base_manifest_db.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data - extract rebracketing type\n",
    "print(f\"Dataset columns: {df.columns.tolist()}\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "\n",
    "# Handle different column formats\n",
    "if \"rebracketing_type\" not in df.columns:\n",
    "    if \"training_data\" in df.columns:\n",
    "        df[\"rebracketing_type\"] = df[\"training_data\"].apply(\n",
    "            lambda x: x.get(\"rebracketing_type\") if isinstance(x, dict) else None\n",
    "        )\n",
    "\n",
    "# Filter to known classes\n",
    "df = df[df[\"concept\"].notna()]\n",
    "df = df[df[\"rebracketing_type\"].isin(class_mapping.keys())]\n",
    "\n",
    "print(f\"\\nFiltered to {len(df)} samples with known rebracketing types\")\n",
    "print(df[\"rebracketing_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding Space Visualization\n",
    "\n",
    "See how different rebracketing types cluster in the learned embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, batch_size=8):  # Reduced batch size to save RAM\n",
    "    \"\"\"Extract embeddings for a list of texts.\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=text_config[\"max_length\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            emb = model.text_encoder(input_ids, attention_mask)\n",
    "            all_embeddings.append(emb.cpu().numpy())\n",
    "        \n",
    "        # Free GPU memory each batch\n",
    "        del input_ids, attention_mask, emb\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings (with caching to avoid RAM issues)\n",
    "EMBEDDINGS_CACHE = \"/content/embeddings_cache.npz\"\n",
    "\n",
    "texts = df[\"concept\"].tolist()\n",
    "labels = df[\"rebracketing_type\"].tolist()\n",
    "colors = df.get(\"rainbow_color\", pd.Series([\"unknown\"] * len(df))).tolist()\n",
    "\n",
    "if os.path.exists(EMBEDDINGS_CACHE):\n",
    "    print(f\"Loading cached embeddings from {EMBEDDINGS_CACHE}\")\n",
    "    cache = np.load(EMBEDDINGS_CACHE, allow_pickle=True)\n",
    "    embeddings = cache[\"embeddings\"]\n",
    "    print(f\"Loaded embeddings shape: {embeddings.shape}\")\n",
    "else:\n",
    "    print(\"Computing embeddings (this will be cached for future runs)...\")\n",
    "    embeddings = get_embeddings(texts)\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Save to cache\n",
    "    np.savez_compressed(EMBEDDINGS_CACHE, embeddings=embeddings)\n",
    "    print(f\"Saved to {EMBEDDINGS_CACHE}\")\n",
    "\n",
    "# Clear some memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE visualization\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"Running TSNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Save TSNE results for reuse\n",
    "np.save(\"/content/tsne_2d.npy\", embeddings_2d)\n",
    "\n",
    "# Plot by rebracketing type\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for rb_type in class_names:\n",
    "    mask = np.array([label == rb_type for label in labels])\n",
    "    if mask.sum() > 0:\n",
    "        ax.scatter(\n",
    "            embeddings_2d[mask, 0],\n",
    "            embeddings_2d[mask, 1],\n",
    "            label=f\"{rb_type} ({mask.sum()})\",\n",
    "            alpha=0.7,\n",
    "            s=50,\n",
    "        )\n",
    "\n",
    "ax.set_xlabel(\"TSNE-1\")\n",
    "ax.set_ylabel(\"TSNE-2\")\n",
    "ax.set_title(\"Embedding Space by Rebracketing Type\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.log({\"tsne_rebracketing\": wandb.Image(fig)})\n",
    "\n",
    "plt.savefig(\"/content/embedding_tsne_rebracketing.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Clean up\n",
    "del tsne\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP visualization (skip if RAM is tight - TSNE is usually sufficient)\n",
    "import umap\n",
    "\n",
    "print(\"Running UMAP...\")\n",
    "reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "embeddings_umap = reducer.fit_transform(embeddings)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for rb_type in class_names:\n",
    "    mask = np.array([label == rb_type for label in labels])\n",
    "    if mask.sum() > 0:\n",
    "        ax.scatter(\n",
    "            embeddings_umap[mask, 0],\n",
    "            embeddings_umap[mask, 1],\n",
    "            label=f\"{rb_type} ({mask.sum()})\",\n",
    "            alpha=0.7,\n",
    "            s=50,\n",
    "        )\n",
    "\n",
    "ax.set_xlabel(\"UMAP-1\")\n",
    "ax.set_ylabel(\"UMAP-2\")\n",
    "ax.set_title(\"Embedding Space by Rebracketing Type (UMAP)\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.log({\"umap_rebracketing\": wandb.Image(fig)})\n",
    "\n",
    "plt.savefig(\"/content/embedding_umap_rebracketing.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Clean up\n",
    "del reducer, embeddings_umap\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot by chromatic color if available\n",
    "if \"rainbow_color\" in df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    color_map = {\n",
    "        \"BLACK\": \"black\",\n",
    "        \"RED\": \"red\",\n",
    "        \"ORANGE\": \"orange\",\n",
    "        \"YELLOW\": \"gold\",\n",
    "        \"GREEN\": \"green\",\n",
    "        \"BLUE\": \"blue\",\n",
    "        \"INDIGO\": \"indigo\",\n",
    "        \"VIOLET\": \"violet\",\n",
    "    }\n",
    "    \n",
    "    for color_name, plot_color in color_map.items():\n",
    "        mask = np.array([c == color_name for c in colors])\n",
    "        if mask.sum() > 0:\n",
    "            ax.scatter(\n",
    "                embeddings_2d[mask, 0],\n",
    "                embeddings_2d[mask, 1],\n",
    "                label=f\"{color_name} ({mask.sum()})\",\n",
    "                color=plot_color,\n",
    "                alpha=0.7,\n",
    "                s=50,\n",
    "            )\n",
    "    \n",
    "    ax.set_xlabel(\"TSNE-1\")\n",
    "    ax.set_ylabel(\"TSNE-2\")\n",
    "    ax.set_title(\"Embedding Space by Chromatic Color\")\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.log({\"tsne_chromatic\": wandb.Image(fig)})\n",
    "    \n",
    "    plt.savefig(\"/content/embedding_tsne_chromatic.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Class Predictions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get predictions (with caching)\nPREDICTIONS_CACHE = \"/content/predictions_cache.npz\"\n\ndef get_predictions(input_texts, batch_size=8):\n    \"\"\"Get predictions for all texts.\"\"\"\n    all_preds = []\n    all_probs = []\n    \n    for i in tqdm(range(0, len(input_texts), batch_size), desc=\"Getting predictions\"):\n        batch_texts = input_texts[i:i+batch_size]\n        \n        encoding = tokenizer(\n            batch_texts,\n            max_length=text_config[\"max_length\"],\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        \n        input_ids = encoding[\"input_ids\"].to(device)\n        attention_mask = encoding[\"attention_mask\"].to(device)\n        \n        with torch.no_grad():\n            logits = model(input_ids, attention_mask)\n            probs = torch.softmax(logits, dim=-1)\n            preds = torch.argmax(logits, dim=-1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n        \n        # Free memory\n        del input_ids, attention_mask, logits, probs, preds\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    \n    return np.array(all_preds), np.array(all_probs)\n\n# Load from cache or compute\nif os.path.exists(PREDICTIONS_CACHE):\n    print(f\"Loading cached predictions from {PREDICTIONS_CACHE}\")\n    cache = np.load(PREDICTIONS_CACHE)\n    predictions = cache[\"predictions\"]\n    probabilities = cache[\"probabilities\"]\n    print(f\"Loaded {len(predictions)} predictions\")\nelse:\n    print(\"Computing predictions (will be cached)...\")\n    predictions, probabilities = get_predictions(texts)\n    np.savez_compressed(PREDICTIONS_CACHE, predictions=predictions, probabilities=probabilities)\n    print(f\"Saved to {PREDICTIONS_CACHE}\")\n\ngc.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Confusion matrix\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Ensure labels match predictions length\nlabel_indices = np.array([class_mapping[lbl] for lbl in labels])\n\nprint(f\"Labels: {len(label_indices)}, Predictions: {len(predictions)}\")\nassert len(label_indices) == len(predictions), \"Mismatch between labels and predictions!\"\n\n# Only use classes that appear in the data\nunique_labels = np.unique(np.concatenate([label_indices, predictions]))\nused_class_names = [class_names[i] for i in unique_labels]\n\ncm = confusion_matrix(label_indices, predictions, labels=unique_labels)\n\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(\n    cm,\n    annot=True,\n    fmt=\"d\",\n    cmap=\"Blues\",\n    xticklabels=used_class_names,\n    yticklabels=used_class_names,\n    ax=ax,\n)\nax.set_xlabel(\"Predicted\")\nax.set_ylabel(\"True\")\nax.set_title(\"Confusion Matrix\")\nplt.tight_layout()\n\nif USE_WANDB:\n    wandb.log({\"confusion_matrix\": wandb.Image(fig)})\n\nplt.savefig(\"/content/confusion_matrix_analysis.png\", dpi=150)\nplt.show()\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(label_indices, predictions, labels=unique_labels, target_names=used_class_names, zero_division=0))\n\n# Get report as dict for wandb\nreport = classification_report(label_indices, predictions, labels=unique_labels, target_names=used_class_names, zero_division=0, output_dict=True)\n\nif USE_WANDB:\n    wandb.log({\n        \"accuracy\": report.get(\"accuracy\", 0),\n        \"macro_f1\": report.get(\"macro avg\", {}).get(\"f1-score\", 0),\n        \"weighted_f1\": report.get(\"weighted avg\", {}).get(\"f1-score\", 0),\n    })"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction confidence distribution\n",
    "max_probs = probabilities.max(axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Overall confidence distribution\n",
    "axes[0].hist(max_probs, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel(\"Prediction Confidence\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Confidence Distribution\")\n",
    "axes[0].axvline(x=0.5, color='red', linestyle='--', label='50%')\n",
    "axes[0].legend()\n",
    "\n",
    "# Confidence by correctness\n",
    "correct = predictions == label_indices\n",
    "axes[1].hist(max_probs[correct], bins=20, alpha=0.7, label=f\"Correct ({correct.sum()})\")\n",
    "axes[1].hist(max_probs[~correct], bins=20, alpha=0.7, label=f\"Incorrect ({(~correct).sum()})\")\n",
    "axes[1].set_xlabel(\"Prediction Confidence\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"Confidence by Correctness\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.log({\"confidence_distribution\": wandb.Image(fig)})\n",
    "\n",
    "plt.savefig(\"/content/confidence_distribution.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Misclassification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "df[\"predicted\"] = [class_names[p] for p in predictions]\n",
    "df[\"confidence\"] = max_probs\n",
    "df[\"correct\"] = df[\"predicted\"] == df[\"rebracketing_type\"]\n",
    "\n",
    "misclassified = df[~df[\"correct\"]]\n",
    "\n",
    "print(f\"Misclassified: {len(misclassified)} / {len(df)} ({100*len(misclassified)/len(df):.1f}%)\")\n",
    "print(\"\\nMost common confusions:\")\n",
    "confusion_counts = misclassified.groupby([\"rebracketing_type\", \"predicted\"]).size().sort_values(ascending=False)\n",
    "print(confusion_counts.head(10))\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.log({\n",
    "        \"misclassification_rate\": len(misclassified) / len(df),\n",
    "        \"num_misclassified\": len(misclassified),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some misclassified examples\n",
    "print(\"\\nExample misclassifications:\")\n",
    "for _, row in misclassified.head(5).iterrows():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"True: {row['rebracketing_type']} | Predicted: {row['predicted']} | Conf: {row['confidence']:.2f}\")\n",
    "    print(f\"Concept: {str(row['concept'])[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log misclassified examples to W&B as a table\n",
    "if USE_WANDB:\n",
    "    misclassified_table = wandb.Table(\n",
    "        columns=[\"concept\", \"true_type\", \"predicted\", \"confidence\"],\n",
    "        data=[\n",
    "            [str(row[\"concept\"])[:500], row[\"rebracketing_type\"], row[\"predicted\"], row[\"confidence\"]]\n",
    "            for _, row in misclassified.head(50).iterrows()\n",
    "        ]\n",
    "    )\n",
    "    wandb.log({\"misclassified_examples\": misclassified_table})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "accuracy = correct.mean()\n",
    "avg_confidence = max_probs.mean()\n",
    "avg_confidence_correct = max_probs[correct].mean()\n",
    "avg_confidence_incorrect = max_probs[~correct].mean() if (~correct).sum() > 0 else 0\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INTERPRETABILITY ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Average confidence: {avg_confidence:.2%}\")\n",
    "print(f\"Avg confidence (correct): {avg_confidence_correct:.2%}\")\n",
    "print(f\"Avg confidence (incorrect): {avg_confidence_incorrect:.2%}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.log({\n",
    "        \"final_accuracy\": accuracy,\n",
    "        \"avg_confidence\": avg_confidence,\n",
    "        \"avg_confidence_correct\": avg_confidence_correct,\n",
    "        \"avg_confidence_incorrect\": avg_confidence_incorrect,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish W&B run\n",
    "if USE_WANDB:\n",
    "    wandb.finish()\n",
    "    print(\"W&B run finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}