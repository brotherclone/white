{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Phase 3.0: DeBERTa Embedding Extraction\n\nExtracts DeBERTa-v3-base embeddings for all training segments.\n\n**What this does:**\n- Embeds `concept` (song-level text, shared across all tracks in a song) -> 768-dim\n- Embeds `lyric_text` (segment-level lyrics, per-segment) -> 768-dim\n- Instrumental segments (no lyrics) get a zero vector for `lyric_embedding`\n\n**Prerequisites:**\n- `/workspace/data/training_segments_metadata.parquet` (0.6 MB metadata — NOT the 15 GB media file)\n- GPU instance (RTX 4090 recommended)\n\n**Output:**\n- `/workspace/data/training_data_with_embeddings.parquet` with `concept_embedding` and `lyric_embedding` columns\n\n**Expected:** 11,605 segments, ~5-10 minutes on RTX 4090"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (safetensors avoids torch.load security issue)\n",
    "!pip install -q transformers pandas pyarrow tqdm sentencepiece tiktoken protobuf safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Embedding extraction will be slow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\n\n# Input: metadata parquet (0.6 MB) — do NOT load the media parquet (15 GB binary audio)\nINPUT_PATH = Path(\"/workspace/data/training_segments_metadata.parquet\")\n\nOUTPUT_PATH = Path(\"/workspace/data/training_data_with_embeddings.parquet\")\n\n# Model - must match Phase 2/4 training\nMODEL_NAME = \"microsoft/deberta-v3-base\"\n\n# Batch size - reduce if OOM\nBATCH_SIZE = 32\n\n# Max sequence length\nMAX_LENGTH = 512\n\nprint(f\"Input: {INPUT_PATH}\")\nprint(f\"Output: {OUTPUT_PATH}\")\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Batch size: {BATCH_SIZE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\nprint(f\"Loading {INPUT_PATH}...\")\ndf = pd.read_parquet(INPUT_PATH)\n\nprint(f\"Loaded {len(df)} segments\")\nprint(f\"Columns: {len(df.columns)}\")\n\n# Check text columns\nhas_concept = df['concept'].notna().sum() if 'concept' in df.columns else 0\nhas_lyrics = df['lyric_text'].notna().sum() if 'lyric_text' in df.columns else 0\nprint(f\"\\nConcept coverage: {has_concept}/{len(df)} ({100*has_concept/len(df):.1f}%)\")\nprint(f\"Lyric text coverage: {has_lyrics}/{len(df)} ({100*has_lyrics/len(df):.1f}%)\")\n\n# Check for existing embeddings\nfor col in ['concept_embedding', 'lyric_embedding']:\n    if col in df.columns:\n        print(f\"\\nWARNING: '{col}' already exists — you may want to skip re-extraction.\")\n\n# Verify rainbow colors\nif 'rainbow_color' in df.columns:\n    print(f\"\\nColors: {sorted(df['rainbow_color'].unique())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Prepare Text Data\n\nTwo text columns to embed:\n- **concept**: Song-level narrative (same for all segments of a song). 100% coverage.\n- **lyric_text**: Segment-level lyrics. ~90% for vocal tracks, 0% for instrumentals (Green, some Yellow/Red)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Concept text (song-level) — 100% coverage\nconcept_texts = df['concept'].fillna(\"\").astype(str).tolist()\n\n# Lyric text (segment-level) — varies by color, 0% for instrumentals\nlyric_texts = df['lyric_text'].fillna(\"\").astype(str).tolist()\n\nconcept_non_empty = sum(1 for t in concept_texts if len(t.strip()) > 0)\nlyric_non_empty = sum(1 for t in lyric_texts if len(t.strip()) > 0)\n\nprint(f\"Concept texts: {concept_non_empty}/{len(concept_texts)} non-empty\")\nprint(f\"Lyric texts:   {lyric_non_empty}/{len(lyric_texts)} non-empty\")\nprint(f\"Lyric-empty segments will get zero vectors for lyric_embedding\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"\\nLoading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"Loading model (using safetensors)...\")\n",
    "# Use safetensors format to avoid torch.load security issue\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, use_safetensors=True)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"\\nModel loaded!\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Extract Embeddings\n\nRuns two passes: one for `concept`, one for `lyric_text`.\nEmpty lyric texts produce zero vectors (instrumental segments)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from tqdm import tqdm\nimport numpy as np\n\nHIDDEN_SIZE = 768  # DeBERTa-v3-base\n\n\ndef extract_embeddings(texts, label=\"\"):\n    \"\"\"Extract CLS token embeddings, using zero vectors for empty texts.\"\"\"\n    embeddings = []\n    num_batches = (len(texts) + BATCH_SIZE - 1) // BATCH_SIZE\n\n    print(f\"Extracting {label} embeddings for {len(texts)} texts in {num_batches} batches...\")\n\n    with torch.no_grad():\n        for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=label):\n            batch_texts = texts[i:i+BATCH_SIZE]\n\n            # Split into non-empty and empty\n            batch_embeddings = np.zeros((len(batch_texts), HIDDEN_SIZE), dtype=np.float32)\n            non_empty_indices = [j for j, t in enumerate(batch_texts) if len(t.strip()) > 0]\n\n            if non_empty_indices:\n                non_empty_texts = [batch_texts[j] for j in non_empty_indices]\n\n                encoded = tokenizer(\n                    non_empty_texts,\n                    max_length=MAX_LENGTH,\n                    padding=\"max_length\",\n                    truncation=True,\n                    return_tensors=\"pt\"\n                )\n\n                input_ids = encoded[\"input_ids\"].to(device)\n                attention_mask = encoded[\"attention_mask\"].to(device)\n\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                non_empty_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n\n                for k, j in enumerate(non_empty_indices):\n                    batch_embeddings[j] = non_empty_emb[k]\n\n                del input_ids, attention_mask, outputs, non_empty_emb\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n            embeddings.extend(batch_embeddings)\n\n    print(f\"  Done: {len(embeddings)} embeddings\")\n    return embeddings\n\n\nconcept_embeddings = extract_embeddings(concept_texts, label=\"concept\")\nlyric_embeddings = extract_embeddings(lyric_texts, label=\"lyric\")\n\n# Verify\nzero_lyrics = sum(1 for e in lyric_embeddings if np.allclose(e, 0))\nprint(f\"\\nZero-vector lyric embeddings (instrumental): {zero_lyrics}/{len(lyric_embeddings)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Add Embeddings to DataFrame"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Adding embeddings to dataframe...\")\n\ndf['concept_embedding'] = concept_embeddings\ndf['lyric_embedding'] = lyric_embeddings\n\n# Flag which segments have real lyric embeddings vs zero vectors\ndf['has_lyric_embedding'] = [len(t.strip()) > 0 for t in lyric_texts]\n\nprint(f\"Columns: {len(df.columns)}\")\nprint(f\"has_lyric_embedding: {df['has_lyric_embedding'].sum()}/{len(df)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving to {OUTPUT_PATH}...\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save\n",
    "df.to_parquet(OUTPUT_PATH)\n",
    "\n",
    "# Check file size\n",
    "size_gb = OUTPUT_PATH.stat().st_size / (1024**3)\n",
    "print(f\"\\nSaved! File size: {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verify Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Verifying saved file...\")\n\ndf_check = pd.read_parquet(OUTPUT_PATH)\n\nprint(f\"Rows: {len(df_check)}\")\nprint(f\"Columns: {len(df_check.columns)}\")\n\nfor col in ['concept_embedding', 'lyric_embedding']:\n    if col in df_check.columns:\n        emb = np.array(df_check[col].iloc[0])\n        print(f\"\\n{col}:\")\n        print(f\"  Shape: {emb.shape}\")\n        print(f\"  Dtype: {emb.dtype}\")\n        print(f\"  Sample: {emb[:5]}\")\n\nif 'has_lyric_embedding' in df_check.columns:\n    print(f\"\\nhas_lyric_embedding: {df_check['has_lyric_embedding'].sum()}/{len(df_check)}\")\n\nif 'rainbow_color' in df_check.columns:\n    print(f\"Colors: {sorted(df_check['rainbow_color'].unique())}\")\n\nprint(\"\\nVerification complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Rename Output (Optional)\n",
    "\n",
    "If you want to replace the original file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Uncomment to rename\n# import shutil\n# \n# FINAL_PATH = Path(\"/workspace/data/training_segments_media.parquet\")\n# BACKUP_PATH = Path(\"/workspace/data/training_data_no_embeddings.parquet\")\n# \n# # Backup original\n# if FINAL_PATH.exists():\n#     shutil.move(FINAL_PATH, BACKUP_PATH)\n#     print(f\"Backed up original to {BACKUP_PATH}\")\n# \n# # Rename new file\n# shutil.move(OUTPUT_PATH, FINAL_PATH)\n# print(f\"Renamed to {FINAL_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"PHASE 3.0: EMBEDDING EXTRACTION COMPLETE\")\nprint(\"=\" * 60)\nprint(\"\")\nprint(f\"Input:  {INPUT_PATH}\")\nprint(f\"Output: {OUTPUT_PATH}\")\nprint(\"\")\nprint(f\"Segments: {len(df)}\")\nprint(f\"Concept embeddings: {len(concept_embeddings)} x 768\")\nprint(f\"Lyric embeddings:   {len(lyric_embeddings)} x 768 ({df['has_lyric_embedding'].sum()} real, {(~df['has_lyric_embedding']).sum()} zero)\")\nprint(f\"File size: {size_gb:.2f} GB\")\nprint(\"\")\nprint(\"Next: Phase 3.1/3.2 — multimodal fusion (CLAP audio + MIDI CNN + text)\")\nprint(\"=\" * 60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}