{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8.6: Multi-Task vs Single-Task Comparison\n",
    "## January 27, 2023 11:15 AM\n",
    "Compares:\n",
    "1. **Single-task classification** (already done: 100% accuracy)\n",
    "2. **Single-task regression** (already done: 98.4% mode acc, 96.9% album acc)\n",
    "3. **Multi-task** (both heads trained simultaneously)\n",
    "\n",
    "Using pre-computed DeBERTa embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Upgrade typing_extensions FIRST, then restart kernel\n",
    "!pip install -q --upgrade 'typing_extensions>=4.12.0'\n",
    "\n",
    "print(\"typing_extensions upgraded. Now restart kernel:\")\n",
    "print(\"  Kernel -> Restart Kernel\")\n",
    "print(\"Then SKIP this cell and run from cell 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Install other dependencies (run this AFTER kernel restart)\n",
    "!pip install -q torch pandas pyarrow scikit-learn scipy tqdm wandb matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb (optional)\n",
    "USE_WANDB = True\n",
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    wandb.init(project=\"white-multitask-comparison\", name=\"task-8.6-comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"data\": {\n",
    "        \"parquet_path\": \"/workspace/data/training_data_with_embeddings.parquet\",\n",
    "        \"embedding_column\": \"embedding\",\n",
    "        \"label_smoothing\": 0.1,\n",
    "        \"train_split\": 0.8,\n",
    "        \"random_seed\": 42,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"embedding_dim\": 768,\n",
    "        \"hidden_dims\": [256, 128],\n",
    "        \"dropout\": 0.3,\n",
    "        \"num_classes\": 8,  # rebracketing types\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\": 30,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"early_stopping_patience\": 7,\n",
    "    },\n",
    "    # Single-task results for comparison\n",
    "    \"baseline\": {\n",
    "        \"single_task_classification_accuracy\": 1.0,\n",
    "        \"single_task_regression_temporal_acc\": 0.984,\n",
    "        \"single_task_regression_spatial_acc\": 0.984,\n",
    "        \"single_task_regression_ontological_acc\": 0.985,\n",
    "        \"single_task_regression_album_acc\": 0.969,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Class mapping for rebracketing types\n",
    "CLASS_MAPPING = {\n",
    "    \"spatial\": 0, \"temporal\": 1, \"causal\": 2, \"perceptual\": 3,\n",
    "    \"memory\": 4, \"ontological\": 5, \"narrative\": 6, \"identity\": 7,\n",
    "}\n",
    "IDX_TO_CLASS = {v: k for k, v in CLASS_MAPPING.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T22:43:41.514441Z",
     "start_time": "2026-01-26T22:43:41.239800Z"
    }
   },
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ndf = pd.read_parquet(CONFIG['data']['parquet_path'])\nprint(f\"Loaded {len(df)} segments\")\n\n# Check for rebracketing_type column\nif 'rebracketing_type' not in df.columns:\n    print(\"WARNING: rebracketing_type not in data, will use 'temporal' as default\")\n    df['rebracketing_type'] = 'temporal'\nelse:\n    print(\"\\nRebracketing type distribution:\")\n    print(df['rebracketing_type'].value_counts())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset with Both Classification and Regression Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftTargetGenerator:\n",
    "    TEMPORAL_MODES = [\"Past\", \"Present\", \"Future\"]\n",
    "    SPATIAL_MODES = [\"Thing\", \"Place\", \"Person\"]\n",
    "    ONTOLOGICAL_MODES = [\"Imagined\", \"Forgotten\", \"Known\"]\n",
    "\n",
    "    def __init__(self, label_smoothing: float = 0.1):\n",
    "        self.smoothing = label_smoothing\n",
    "\n",
    "    def to_soft_target(self, label, mode_list):\n",
    "        if label is None or pd.isna(label) or str(label) == \"None\":\n",
    "            return np.array([1/3, 1/3, 1/3])\n",
    "        target = np.zeros(len(mode_list))\n",
    "        try:\n",
    "            target[mode_list.index(str(label))] = 1.0\n",
    "        except ValueError:\n",
    "            return np.array([1/len(mode_list)] * len(mode_list))\n",
    "        return (1 - self.smoothing) * target + self.smoothing * (1/len(mode_list))\n",
    "\n",
    "    def generate(self, row):\n",
    "        temporal = self.to_soft_target(row.get(\"rainbow_color_temporal_mode\"), self.TEMPORAL_MODES)\n",
    "        spatial = self.to_soft_target(row.get(\"rainbow_color_objectional_mode\"), self.SPATIAL_MODES)\n",
    "        ontological = self.to_soft_target(row.get(\"rainbow_color_ontological_mode\"), self.ONTOLOGICAL_MODES)\n",
    "        is_black = all(pd.isna(x) or x is None or str(x) == \"None\" \n",
    "                       for x in [row.get(\"rainbow_color_temporal_mode\"), \n",
    "                                 row.get(\"rainbow_color_objectional_mode\"),\n",
    "                                 row.get(\"rainbow_color_ontological_mode\")])\n",
    "        confidence = np.array([0.0 if is_black else 1.0])\n",
    "        return {\"temporal\": temporal, \"spatial\": spatial, \"ontological\": ontological, \"confidence\": confidence}\n",
    "\n",
    "\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, df, embedding_col, class_mapping, label_smoothing=0.1):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.embedding_col = embedding_col\n",
    "        self.class_mapping = class_mapping\n",
    "        self.target_gen = SoftTargetGenerator(label_smoothing)\n",
    "        \n",
    "        print(\"Pre-computing targets...\")\n",
    "        self.regression_targets = [self.target_gen.generate(self.df.iloc[i]) for i in tqdm(range(len(df)))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        embedding = row[self.embedding_col]\n",
    "        if isinstance(embedding, list):\n",
    "            embedding = np.array(embedding)\n",
    "        \n",
    "        # Classification target\n",
    "        rtype = str(row.get('rebracketing_type', 'temporal')).lower()\n",
    "        class_idx = self.class_mapping.get(rtype, 1)  # default to temporal\n",
    "        \n",
    "        # Regression targets\n",
    "        reg = self.regression_targets[idx]\n",
    "        \n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float32),\n",
    "            \"class_label\": torch.tensor(class_idx, dtype=torch.long),\n",
    "            \"temporal_target\": torch.tensor(reg[\"temporal\"], dtype=torch.float32),\n",
    "            \"spatial_target\": torch.tensor(reg[\"spatial\"], dtype=torch.float32),\n",
    "            \"ontological_target\": torch.tensor(reg[\"ontological\"], dtype=torch.float32),\n",
    "            \"confidence_target\": torch.tensor(reg[\"confidence\"], dtype=torch.float32),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Task Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskHead(nn.Module):\n",
    "    \"\"\"Combined classification + regression head.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=768, hidden_dims=[256, 128], num_classes=8, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared layers\n",
    "        layers = []\n",
    "        in_dim = embedding_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.extend([nn.Linear(in_dim, h), nn.ReLU(), nn.Dropout(dropout)])\n",
    "            in_dim = h\n",
    "        self.shared = nn.Sequential(*layers)\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.classifier = nn.Linear(hidden_dims[-1], num_classes)\n",
    "        self.temporal_head = nn.Linear(hidden_dims[-1], 3)\n",
    "        self.spatial_head = nn.Linear(hidden_dims[-1], 3)\n",
    "        self.ontological_head = nn.Linear(hidden_dims[-1], 3)\n",
    "        self.confidence_head = nn.Linear(hidden_dims[-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.shared(x)\n",
    "        return {\n",
    "            \"class_logits\": self.classifier(h),\n",
    "            \"temporal\": F.softmax(self.temporal_head(h), dim=-1),\n",
    "            \"spatial\": F.softmax(self.spatial_head(h), dim=-1),\n",
    "            \"ontological\": F.softmax(self.ontological_head(h), dim=-1),\n",
    "            \"confidence\": torch.sigmoid(self.confidence_head(h)),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Task Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self, cls_weight=1.0, reg_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.cls_weight = cls_weight\n",
    "        self.reg_weight = reg_weight\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.kl = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        self.bce = nn.BCELoss()\n",
    "        \n",
    "    def forward(self, preds, targets):\n",
    "        # Classification loss\n",
    "        cls_loss = self.ce(preds[\"class_logits\"], targets[\"class_label\"])\n",
    "        \n",
    "        # Regression losses\n",
    "        t_loss = self.kl(preds[\"temporal\"].clamp(min=1e-8).log(), targets[\"temporal_target\"])\n",
    "        s_loss = self.kl(preds[\"spatial\"].clamp(min=1e-8).log(), targets[\"spatial_target\"])\n",
    "        o_loss = self.kl(preds[\"ontological\"].clamp(min=1e-8).log(), targets[\"ontological_target\"])\n",
    "        c_loss = self.bce(preds[\"confidence\"], targets[\"confidence_target\"])\n",
    "        \n",
    "        reg_loss = t_loss + s_loss + o_loss + 0.5 * c_loss\n",
    "        total = self.cls_weight * cls_loss + self.reg_weight * reg_loss\n",
    "        \n",
    "        return total, {\"cls\": cls_loss, \"temporal\": t_loss, \"spatial\": s_loss, \n",
    "                       \"ontological\": o_loss, \"confidence\": c_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, targets):\n",
    "    metrics = {}\n",
    "    \n",
    "    # Classification accuracy\n",
    "    cls_pred = preds[\"class_logits\"].argmax(dim=-1).cpu().numpy()\n",
    "    cls_true = targets[\"class_label\"].cpu().numpy()\n",
    "    metrics[\"classification_accuracy\"] = accuracy_score(cls_true, cls_pred)\n",
    "    metrics[\"classification_f1\"] = f1_score(cls_true, cls_pred, average=\"macro\")\n",
    "    \n",
    "    # Regression mode accuracies\n",
    "    for dim in [\"temporal\", \"spatial\", \"ontological\"]:\n",
    "        pred_mode = preds[dim].argmax(dim=-1).cpu().numpy()\n",
    "        true_mode = targets[f\"{dim}_target\"].argmax(dim=-1).cpu().numpy()\n",
    "        metrics[f\"{dim}_mode_accuracy\"] = (pred_mode == true_mode).mean()\n",
    "    \n",
    "    # Album accuracy\n",
    "    pred_t = preds[\"temporal\"].argmax(dim=-1)\n",
    "    pred_s = preds[\"spatial\"].argmax(dim=-1)\n",
    "    pred_o = preds[\"ontological\"].argmax(dim=-1)\n",
    "    true_t = targets[\"temporal_target\"].argmax(dim=-1)\n",
    "    true_s = targets[\"spatial_target\"].argmax(dim=-1)\n",
    "    true_o = targets[\"ontological_target\"].argmax(dim=-1)\n",
    "    correct = (pred_t == true_t) & (pred_s == true_s) & (pred_o == true_o)\n",
    "    metrics[\"album_accuracy\"] = correct.float().mean().item()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        emb = batch[\"embedding\"].to(device)\n",
    "        targets = {k: v.to(device) for k, v in batch.items() if k != \"embedding\"}\n",
    "        \n",
    "        preds = model(emb)\n",
    "        loss, _ = criterion(preds, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = {k: [] for k in [\"class_logits\", \"temporal\", \"spatial\", \"ontological\", \"confidence\"]}\n",
    "    all_targets = {k: [] for k in [\"class_label\", \"temporal_target\", \"spatial_target\", \"ontological_target\", \"confidence_target\"]}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            emb = batch[\"embedding\"].to(device)\n",
    "            targets = {k: v.to(device) for k, v in batch.items() if k != \"embedding\"}\n",
    "            \n",
    "            preds = model(emb)\n",
    "            loss, _ = criterion(preds, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            for k in all_preds:\n",
    "                all_preds[k].append(preds[k])\n",
    "            for k in all_targets:\n",
    "                all_targets[k].append(targets[k])\n",
    "    \n",
    "    preds_cat = {k: torch.cat(v) for k, v in all_preds.items()}\n",
    "    targets_cat = {k: torch.cat(v) for k, v in all_targets.items()}\n",
    "    \n",
    "    metrics = compute_metrics(preds_cat, targets_cat)\n",
    "    metrics[\"val_loss\"] = total_loss / len(loader)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(df)), train_size=CONFIG[\"data\"][\"train_split\"], \n",
    "    random_state=CONFIG[\"data\"][\"random_seed\"]\n",
    ")\n",
    "\n",
    "train_df = df.iloc[train_idx]\n",
    "val_df = df.iloc[val_idx]\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiTaskDataset(train_df, CONFIG[\"data\"][\"embedding_column\"], CLASS_MAPPING)\n",
    "val_dataset = MultiTaskDataset(val_df, CONFIG[\"data\"][\"embedding_column\"], CLASS_MAPPING)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"training\"][\"batch_size\"], shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"training\"][\"batch_size\"], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Multi-Task Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiTaskHead(\n",
    "    embedding_dim=CONFIG[\"model\"][\"embedding_dim\"],\n",
    "    hidden_dims=CONFIG[\"model\"][\"hidden_dims\"],\n",
    "    num_classes=CONFIG[\"model\"][\"num_classes\"],\n",
    "    dropout=CONFIG[\"model\"][\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "criterion = MultiTaskLoss(cls_weight=1.0, reg_weight=1.0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"training\"][\"learning_rate\"], \n",
    "                              weight_decay=CONFIG[\"training\"][\"weight_decay\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MULTI-TASK TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_metrics = {}\n",
    "patience = 0\n",
    "history = []\n",
    "\n",
    "for epoch in range(CONFIG[\"training\"][\"epochs\"]):\n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['training']['epochs']}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_metrics = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    history.append({\"train_loss\": train_loss, **val_metrics})\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.log({\"train_loss\": train_loss, **val_metrics, \"epoch\": epoch})\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_metrics['val_loss']:.4f}\")\n",
    "    print(f\"  Classification Acc: {val_metrics['classification_accuracy']:.3f}\")\n",
    "    print(f\"  Temporal Mode Acc: {val_metrics['temporal_mode_accuracy']:.3f}\")\n",
    "    print(f\"  Spatial Mode Acc: {val_metrics['spatial_mode_accuracy']:.3f}\")\n",
    "    print(f\"  Ontological Mode Acc: {val_metrics['ontological_mode_accuracy']:.3f}\")\n",
    "    print(f\"  Album Acc: {val_metrics['album_accuracy']:.3f}\")\n",
    "    \n",
    "    scheduler.step(val_metrics[\"val_loss\"])\n",
    "    \n",
    "    if val_metrics[\"val_loss\"] < best_val_loss:\n",
    "        best_val_loss = val_metrics[\"val_loss\"]\n",
    "        best_metrics = val_metrics.copy()\n",
    "        patience = 0\n",
    "        torch.save(model.state_dict(), \"/workspace/output/multitask_best.pt\")\n",
    "        print(\"  Saved best model\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= CONFIG[\"training\"][\"early_stopping_patience\"]:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TASK 8.6: MULTI-TASK vs SINGLE-TASK COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "baseline = CONFIG[\"baseline\"]\n",
    "\n",
    "comparison = {\n",
    "    \"Metric\": [\n",
    "        \"Classification Accuracy\",\n",
    "        \"Temporal Mode Accuracy\",\n",
    "        \"Spatial Mode Accuracy\", \n",
    "        \"Ontological Mode Accuracy\",\n",
    "        \"Album Accuracy\",\n",
    "    ],\n",
    "    \"Single-Task\": [\n",
    "        baseline[\"single_task_classification_accuracy\"],\n",
    "        baseline[\"single_task_regression_temporal_acc\"],\n",
    "        baseline[\"single_task_regression_spatial_acc\"],\n",
    "        baseline[\"single_task_regression_ontological_acc\"],\n",
    "        baseline[\"single_task_regression_album_acc\"],\n",
    "    ],\n",
    "    \"Multi-Task\": [\n",
    "        best_metrics[\"classification_accuracy\"],\n",
    "        best_metrics[\"temporal_mode_accuracy\"],\n",
    "        best_metrics[\"spatial_mode_accuracy\"],\n",
    "        best_metrics[\"ontological_mode_accuracy\"],\n",
    "        best_metrics[\"album_accuracy\"],\n",
    "    ],\n",
    "}\n",
    "\n",
    "comparison[\"Delta\"] = [m - s for s, m in zip(comparison[\"Single-Task\"], comparison[\"Multi-Task\"])]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(comparison[\"Metric\"]))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison[\"Single-Task\"], width, label=\"Single-Task\", color=\"steelblue\")\n",
    "bars2 = ax.bar(x + width/2, comparison[\"Multi-Task\"], width, label=\"Multi-Task\", color=\"coral\")\n",
    "\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Task 8.6: Single-Task vs Multi-Task Performance Comparison\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([m.replace(\" \", \"\\n\") for m in comparison[\"Metric\"]], fontsize=9)\n",
    "ax.legend()\n",
    "ax.set_ylim(0.9, 1.02)\n",
    "ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    ax.annotate(f'{bar.get_height():.3f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "for bar in bars2:\n",
    "    ax.annotate(f'{bar.get_height():.3f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/workspace/output/multitask_comparison.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "avg_single = np.mean(comparison[\"Single-Task\"])\n",
    "avg_multi = np.mean(comparison[\"Multi-Task\"])\n",
    "avg_delta = np.mean(comparison[\"Delta\"])\n",
    "\n",
    "print(f\"\\nAverage Single-Task Performance: {avg_single:.3f}\")\n",
    "print(f\"Average Multi-Task Performance: {avg_multi:.3f}\")\n",
    "print(f\"Average Delta: {avg_delta:+.3f}\")\n",
    "\n",
    "if avg_delta > 0.01:\n",
    "    verdict = \"Multi-task IMPROVES performance\"\n",
    "elif avg_delta < -0.01:\n",
    "    verdict = \"Multi-task HURTS performance (task interference)\"\n",
    "else:\n",
    "    verdict = \"Multi-task performs SIMILARLY to single-task\"\n",
    "\n",
    "print(f\"\\nVerdict: {verdict}\")\n",
    "print(\"\\nNote: Both approaches achieve near-ceiling performance (>96%).\")\n",
    "print(\"The embeddings already capture the task-relevant information effectively.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    \"task\": \"8.6 - Multi-Task vs Single-Task Comparison\",\n",
    "    \"single_task\": {k: float(v) for k, v in zip(comparison[\"Metric\"], comparison[\"Single-Task\"])},\n",
    "    \"multi_task\": {k: float(v) for k, v in zip(comparison[\"Metric\"], comparison[\"Multi-Task\"])},\n",
    "    \"delta\": {k: float(v) for k, v in zip(comparison[\"Metric\"], comparison[\"Delta\"])},\n",
    "    \"average_single\": float(avg_single),\n",
    "    \"average_multi\": float(avg_multi),\n",
    "    \"average_delta\": float(avg_delta),\n",
    "    \"verdict\": verdict,\n",
    "    \"epochs_trained\": len(history),\n",
    "}\n",
    "\n",
    "with open(\"/workspace/output/multitask_comparison_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\nResults saved to /workspace/output/multitask_comparison_results.json\")\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.log({\"comparison_chart\": wandb.Image(\"/workspace/output/multitask_comparison.png\")})\n",
    "    wandb.log(results)\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"\\nTask 8.6 Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
